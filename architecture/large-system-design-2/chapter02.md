
# 2장. 주변 친구

### 목표: `모바일 앱 기능을 지원`하는 `규모 확장이 용이`한 `백엔드 시스템 설계`

- 인근의 친구 목록을 보여주는 시스템
- 근접성 서비스와 유사해보일 수 있지만 주변 친구의 위치는 계속 바뀔 수 있음

<br>

## 1단계: 문제 이해 및 설계 범위 확정

- 주변? → 5마일(8km, 직선거리 기준)
- 10억명 유저 → DAU 10% → 1억명
- 동시 사용자 → 10% → 천만명
- 이동 이력 보관 → ML 등 다양한 용도로 활용 예정
- 10분 비활성화 시, 사라짐
- 이 단계에서 사생활 및 데이터 보호법은 고려 X

### 기능 요구사항

- 사용자는 모바일 앱에서 주변 친구 확인
    - 친구까지의 거리 표현
    - 마지막으로 갱신된 시간 표현
- 친구 목록은 정해진 시간(초단위)마다 갱신

### 비기능 요구사항

- 낮은 지연시간
- 안정성: 몇 개의 데이터 유실은 용인
- 결과적 일관성(eventual consistency):
    - 강한 일관성까지는 필요 X
    - 복제본의 데이터가 원본과 동일하게 변경되기까지 몇 초 정도 걸리는 것은 용인

### 개략적 규모 추정

- 주변 친구 → 5마일(8km), 직선 거리
- 30초마다 위치 정보 갱신 → 사람의 평균 걷는 속도 고려
- 전체 유저 → 10억명
- 유저의 10%가 해당 서비스 능동 사용 → DAU 1억명
- 동시 사용자 → DAU의 10% → 천만명이 동시 사용
- 사용자 한 명의 평균 친구 수는 400명으로 가정 + 모든 친구가 주변 친구를 사용 가정
- 페이지당 20명의 주변 친구 표시 + 사용자 요청 시, 추가 출력

### QPS 계산

- DAU: 1억명
- 동시 접속 사용자: 천만명
- 사용자는 30초마다 자기 위치 전송
- 위치 정보 갱신 QPS

$QPS = 10,000,000 / 30 =~ 334,000$

<br>

## 2단계: 개략적 설계안 제시 및 동의 구하기

> 위치 정보를 모든 친구에게 전송해야하는 요구사항으로 인해 C/S 사이의 통신 프로토콜로 단순한 HTTP를 사용하지 못하게 될 수도 있는 제약사항 존재
> 
- 개략적 설계
- API 설계
- 데이터 모델

### 개략적 설계안

- 메시지의 효과적 전송을 가능하게할 설계안 필요
- 이론적으로는 순수한 peer-to-peer 방식으로도 해결 가능한 문제
    - 활성 상태인 근방 모든 친구와 항구적 통신 상태를 유지
- 모바일 디바이스의 경우, 통신 연결 상태가 좋지 않을 수도 있고 사용할 수 있는 전력도 충분치 않을 수 있음
- 실용적인 해결방안으로는 `공용 백엔드를 사용`하는 것
- 백엔드의 역할:
    - 모든 활성 상태 사용자의 위치 변화 내역 수신
    - 사용자의 위치 변경 내역을 수신할 때마다 해당 사용자의 모든 활성 상태 친구를 찾아서 그 친구들의 단말로 변경 내역 전달
    - 두 사용자 사이의 거리가 특정 임계치보다 먼 경우에는 변경 내역 전송 X
- 하지만 이런 구조에도 문제점이 존재:
    - 큰 규모에서 적용하기 쉽지 않음
        - 활성 상태의 동시 접속자 천만 명
        - 모두가 자기 위치 정보를 30초마다 갱신
        - 초당 334,000 번의 정보 갱신 처리 필요
        - 조건상 사용자 한 명이 400명의 친구 * 10%가 인근에서 활성화 상태라고 가정 → 334,000 * 400 * 0.1 = 1400만 건의 위치 정보 갱신 요청 필요

### 설계안

- 소규모 백엔드를 위한 개략적 설계안
- 중요 컴포넌트:
    - 로드밸런서:
    - RESTful API 서버:
        - Stateless API 서버
        - 통상적인 요청/응답 트래픽 처리
        - 모바일 사용자 → HTTP → 로드밸런서 → API 서버 → 사용자 데이터베이스
    - 양방향 stateful 웹소켓 서버:
        - 친구의 위치 정보 변경을 거의 실시간에 가깝게 처리하는 stateful 서버 클러스터
        - 사용자는 클러스터 중 한 서버와 웹소켓 연결을 지속적으로 유지 (위치 변경 시, 해당 연결 사용)
        - 초기화도 담당 → 온라인 상태의 모든 주변 친구 위치를 해당 클라이언트로 전송하는 역할
    - 레디스 위치 정보 캐시:
        - 활성 상태 사용자의 가장 최근 위치 정보를 캐시하는 데 사용
        - 캐시 항목에 TTL 필드 존재 → 해당 기간이 지나면 해당 사용자는 비활성 상태로 변경 및 캐시에서 위치 정보 삭제
        - 레디스 이외에 TTL을 지원하는 키-밸류 저장소를 캐시로 활용 가능
    - 사용자 데이터베이스:
        - 사용자 데이터 및 사용자의 친구 관계 정보 저장
        - RDBS, NoSQL 모두 사용 가능
    - 위치 이동 이력 데이터베이스:
        - 사용자의 위치 변동 이력 보관 (주변 친구 표시와 직접 관계된 기능 X)
    - 레디스Pub/Sub 서버:
        - 초경량 메시지 버스
        - 새로운 채널 생성에 리소스가 아주 적게듬 (GB급 메모리 → 수백만 개의 채널 생성 가능)
        - 웹소켓 서버를 통해 수신한 특정 사용자의 위치 정보 변경 이벤트는 해당 사용자에게 배정된 채널에서 발행
        - 특정 사용자의 위치가 바뀌면 해당 사용자의 모든 친구의 웹소켓 연결 핸들러가 호출됨
        - 호출 → 거리 계산 → 검색 반경 이내면 갱신 위치 & 갱신 시각(timestamp)을 웹소켓 연결을 통해 친구의 클라이언트 앱으로 전송

### 주기적 위치 갱신

> 모바일 클라이언트는 유지되는 웹소켓 연결을 통해, 주기적으로 위치 변경 내역 전송
> 
- 모바일 클라이언트 → 위치 변경 정보 → 로드밸런서
- 로드밸런서 → 해당 클라이언트—웹소켓 연결 → 웹소켓 서버
- 웹소켓 서버 → 해당 이벤트 → 위치 이동 이력 데이터베이스에 저장
- 웹소켓 서버 → 새 위치 정보 → 캐시에 보관 (TTL 갱신)
- 웹소켓 서버 → 웹소켓 연결 핸들러에 위치 정보 반영 (거리 계산 과정에 사용)
- 웹소켓 서버 → 해당 사용자 채널에 새 위치 발행 → 레디스 펍/섭 서버
- 레디스 펍/섭 서버 → 발행된 새로운 위치 변경 이벤트 → 모든 구독자(웹소켓 이벤트 핸들러)에게 브로드캐스트 (해당 사용자의 온라인 상태 친구들에게 전달) → 각 구독자의 웹소켓 핸들러는 변경 이벤트 수신
- 메시지를 받은 웹소켓 서버 → 거리 계산 (새 위치를 보낸 사용자 ↔ 메시지를 받은 사용자)
- 거리 계산 결과가 반경 내일 경우 → 새 위치 & 타임스탬프 → 해당 구독자의 클라이언트 앱으로 전송

### API 설계

- 웹소켓:
    - 사용자는 웹소켓 프로토콜을 통해 위치 정보 변경 내역을 전송하고 수신
- [서버 API] 주기적인 위치 정보 갱신:
    - 요청: 클라이언트는 위도, 경도, 시각 정보를 전송
    - 응답: 없음
- [클라이언트 API] 클라이언트가 갱신된 친구 위치를 수신하는 데 사용할 API:
    - 전송되는 데이터: 친구 위치 데이터와 변경된 시각을 나타내는 타임스탬프
- [서버 API] 웹소켓 초기화 API:
    - 요청: 클라이언트는 위도, 경도, 시각 정보를 전송
    - 응답: 클라이언트는 자기 친구들의 위치 데이터를 수신
- [클라이언트 API] 새 친구 구독 API:
    - 요청: 웹소켓 서버는 친구 ID 전송
    - 응답: 가장 최근의 위도, 경도, 시각 정보 전송
- [클라이언트 API] 구독 해지 API:
    - 요청: 웹 소켓 서버는 친구 ID 전송
    - 응답: 없음
- HTTP 요청:
    - API 서버는 친구를 추가/삭제하거나 사용자 정보를 갱신하는 등의 작업 처리 필요


### 데이터 모델

- 사용자 데이터베이스
- 위치 정보 캐시:
    - 주변 친구 기능을 켠 활성 상태 친구의 가장 최근 위치 보관
    - 해당 설계안에서는 레디스를 사용하여 캐시 구현
        
        
        | 키 | 값 |
        | --- | --- |
        | 사용자 ID | {위도, 경도, 시각} |
    - 위치 정보 저장에 데이터베이스를 사용하지 않는 이유?:
        - 사용자의 `현재 위치`만 사용 → 사용자 위치는 하나만 보관하면 충분 → 레디스가 적합함!
        - 레디스는 읽기 및 쓰기 연산 속도가 매우 빠름
        - TTL을 지원하므로 활성 상태가 아닌 사용자 정보를 자동으로 제거 가능
        - 레디스 서버 하나에 장애가 발생 → 다른 새 서버로 변경 → 갱신된 위치 정보가 캐싱 되기를 기다리면 됨(warmed up)
- 위치 이동 이력 데이터베이스:
    - 사용자의 위치 정보 변경 이력을 아래의 스키마를 따르는 테이블에 저장
        
        
        | user_id | latitude | longitude | timestamp |
        | --- | --- | --- | --- |
    - 막대한 쓰기 연산 부하를 감당할 수 있고 수평적 규모 확장이 가능한 데이터베이스가 필요
    - 카산드라가 이러한 요구에 잘 부합함
    - RDB도 사용할 수는 있으나 이력 데이터의 양이 서버 한 대에 보관하기에는 너무 많을 수 있음 → 샤딩 필요 → 사용자 ID를 기준으로 샤딩이 가장 기본 → 부하를 모든 샤드에 고르게 분산시킬 수 있고 DB 운영 관리도 간편해짐


## 3단계: 상세 설계

### 중요 구성요소별 규모 확장성

- API 서버:
    - RESTful API 서버 + stateless 서버 → 방법이 다양함
        - 로드 밸런싱
        - 수평 확장
        - 캐싱
        - MSA
        - 비동기
        - 등등
- 웹소켓 서버:
    - 웹소켓 클러스터도 사용률에 따라 규모를 자동으로 늘리는 것은 그다지 어렵지 않음
    - 하지만 웹소켓의 경우 statefule 서버 → 기존 서버를 제거할 때 주의 필요 → 세션이 강제로 끊길 수 있음
        - 로드밸런서의 draining 활용
    - 비슷한 이유로 새로운 버전의 애플리케이션 소프트웨어를 설치할 때도 주의 필요
    - Stateful 서버 클러스터의 규모를 자동으로 확장하기 위해선 좋은 로드밸런서 필요


### 클라이언트 초기화

- 모바일 클라이언트 기동 시 `지속성 웹소켓 연결`을 맺음
    - 연결이 오랜시간 유지
    - 대부분의 현대적 프로그래밍 언어는 이런 연결 유지에 많은 메모리를 필요로 하지 않음
- 사용자의 위치 정보를 수시한 웹소켓 연결 핸들러는 아래의 작업을 수행
    - 위치 정보 캐시에 보관된 해당 사용자의 위치 갱신
    - 해당 위치 정보는 뒤이은 계산 과정에 이용되므로, 연결 핸들러 내의 변수에 저장
    - 사용자 데이터베이스에서 사용자의 모든 친구 정보를 가져옴
    - 위치 정보 캐시에 일괄(batch) 요청을 보내 모든 친구의 위치를 한 번에 가져옴
    - 캐시에 보관하는 모든 항목의 TTL은 비활성화 타임아웃 시간과 동일한 값으로 설정 → 비활성화 친구의 위치는 캐시에 존재 X
    - 캐시가 돌려준 친구 위치 각각에 대해, 웹소켓 서버는 해당 친구와 사용자 사이의 거리를 계산 → 해당 거리가 검색 반경 이내면 해당 친구의 상세 정보, 위치 그리고 해당 위치가 마지막으로 확인된 시각을 웹소켓 연결을 통해 클라이언트에 반환
    - 웹소켓 서버는 각 친구의 레디스 서버 펍/섭 채널을 구독
    - 사용자의 현재 위치를 레디스 펍/섭 서버의 전용 채널을 통해 모든 친구에게 전송

### 사용자 데이터베이스

- 두 가지 종류의 데이터가 보관
    - 사용자 상세 정보:
        - 사용자 ID (`샤딩 시 기준`)
        - 사용자 이름
        - 프로필 이미지 URL
        - 등등
    - 친구 관계 데이터

### 위치 정보 캐시

- 활성화 샅애 사용자의 위치 정보를 캐시하기 위해 레디스 활용
- 각 항목의 키에는 TTL 설정 → 해당 사용자의 위치 정보가 갱신될 때마다 초기화 → `최대 메모리 사용량은 일정 한도 아래로 유지`
- 캐시할 데이터는 쉽게 샤딩 가능
    - 각 사용자의 위치 정보는 서로 독립적인 데이터 → 사용자 ID를 기준으로 여러 서버에 샤딩하면 부하를 고르게 분배 가능
- 가용성을 높이기 위해, 각 샤드에 보관하는 위치 정보를 standby 노드에 복제 → primary 노드 장애 시 승격시켜 장애시간 단축 가능


### 레디스 펍/섭 서버

- 해당 설계안은 펍/섭 서버를 모든 온라인 친구에게 보내는 위치 변경 내역 메시지의 라우팅 계층으로 활용
- 구독자가 없는 채널로 전송된 메시지는 드랍 → 이 과정에서 서버 부하는 거의 없음
- 채널 하나를 유지하기 위해서는 구독자 관계를 추적하기 위한 `해시 테이블`과 `연결 리스트` 필요
    - 소량의 메모리만 사용
    - 오프라인 사용자 → 어떤 변경도 없는 채널 → CPI 자원을 전혀 사용하지 않음
- 활용 방안:
    - 주변 친구 기능을 활용하는 모든 사용자에게 채널 하나씩 부여
    - 해당 기능을 사용하는 사용자의 앱은 초기화 시에 모든 친구의 채널과 구독 관계 설정
    - 친구의 상태는 고려 X
        - 활성화 상태로 바뀐 친구의 채널을 구독하거나 비활성화된 상태가 된 친구의 채널을 구독 중단하는 등의 작업이 필요 없음 → `설계가 단순해짐`
- 얼마나 많은 레디스 펍/섭 서버가 필요할까?
    - 메모리 사용량:
        - 필요 채널 수: 1억개
        - 한 사용자의 활성화 상태 친구 가운데 100명이 주변 친구 기능을 사용한다고 가정
        - 구독자 한 명을 추적하기 위해 내부 해시 테이블과 연결 리스트에 20바이트 상당의 포인터들 저장
        - $1억 * 20Byte * 100명의 친구 / 10^9 = 200GB$
    - CPU 사용량:
        - 펍/섭 서버가 구독자에게 전송해야하는 위치 정보 업데이트의 양 → 초당 1400만 건
        - 보수적으로 기가비트 네트워크 카드를 탑재한 현대적 아키텍처의 서버 한 대로 감당 가능한 구독자 수 → 100,000 가정
        - $1400만 / 100,000 = 140대$ (보수적인 계산, 실제는 훨씬 적을 수 있음)
    - 레디스 펍/섭 서버의 병목은 메모리가 아닌 `CPU 사용량`
    - 본 설계안의 규모를 고려했을 때, 분산 레디스 펍/섭 클러스터 필요

### 분산 레디스 펍/섭 서버 클러스터

- 수백 대의 레디스 서버에 채널을 분산할 방법은 무엇일까?
    - 모든 채널은 서로 독립적 → 메시지를 발행한 사용자 ID를 기준으로 펍/섭 서버들을 샤딩 가능
- Service discovery 컴포넌트(ex. etcd, ZooKeeper)를 문제 해결에 활용
    - 가용한 서버 목록을 유지하는 기능 및 해당 목록을 갱신하는데 필요한 UI / API
        - 설정 데이터를 보관하기 위한 소규모의 키-밸류 저장소로 이해하자
    - 웹소켓 서버 → 값에 명시된 레디스 펍/섭 서버에서 발생한 변경 내역을 구독할 수 있도록 하는 기능
- 웹소켓 서버는 해시 링을 참조하여 메시지를 발행할 레디스 펍/섭 서버를 선정
- 웹소켓 서버는 해당 서버가 관리하는 사용자 채널에 위치 정보 변경 내역 발생

### 레디스 펍/섭 서버 클러스터의 규모 확장 고려사항

- 레디스 펍/섭 서버 클러스터의 규모를 늘리려면 어떻게 해야할까?
    - 트래픽 패턴에 따라?
        - Stateless 서버로 구성된 클러스터에 널리 사용
        - 위험성이 낮음
        - 비용 절감에도 효율적
- 펍/섭 채널에 전송되는 메시지는 메모리나 디스크에 지속적으로 보관되지 않음
    - 채널의 모든 구독자에게 전송되고 나서 바로 삭제
    - 구독자가 아예 없는 경우 그냥 지워짐
    - 펍/섭 채널을 통해 처리되는 데이터는 stateless로 볼 수 있음
- 펍/섭 서버는 채널에 대한 상태 정보를 보관
    - 상태 정보의 핵심: 각 채널의 구독자 목록
    - 특정한 채널을 담당하던 펍/섭 서버를 교체하거나 해시 링에서 제거하는 경우 채널은 다른 서버로 이동시켜야함 → 해당 채널의 모든 구독자에게 해당 사실을 알려야함
        - 이 과정을 통해 기존 채널에 대한 구독 관계를 해지하고 새 서버에 마련된 대체 채널을 다시 구독
        - 펍/섭은 stateful 서버로 볼 수 있음
- 레디스 펍/섭 서버 클러스터는 stateful 서버 클러스터로 취급하는 것이 바람직
    - 이러한 서버 클러스터의 규모를 늘리거나 줄이는 것은 운영 부담과 위험이 큼
    - 주의 깊게 계획하고 진행 필요
    - 일반적으로 어느정도 여유를 두고 오버 프로비저닝함
    - 생각해볼만한 점들
        - 클러스터의 크기를 조정하면 많은 채널이 같은 해시 링 위의 다른 여러 서버로 이동
        - 서비스 디스커버리 컴포넌트가 모든 웹소켓 서버에 해시 링 갱신을 알림 → 엄청난 재구독 요청 발생
        - 재구독 요청을 처리할 때, 클라이언트가 보내는 위치 정보 변경 메시지의 누락이 발생할 수 있음
        - 서비스의 상태가 불안정해질 가능성이 있음
        - 결국 클러스터 크기 조정은 하루 중 시스템 부하가 가장 낮은 시간을 골라서 진행해야함
- 클러스터 크기 조절 방안:
    - 새로운 링 크기 계산 → 크기가 늘어날 경우 새 서버 준비
    - 해시 링의 키 값에 매달린 값을 새로운 내용으로 갱신
    - 대시보드 모니터링 → CPU 사용량 체크



























